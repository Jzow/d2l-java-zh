{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 通过时间反向传播\n",
    ":label:`sec_bptt`\n",
    "\n",
    "到目前为止，我们已经多次提到\n",
    "*爆炸梯度*，\n",
    "*消失梯度*，\n",
    "以及*需要分离*RNN的梯度.\n",
    "例如， 在 :numref:`sec_rnn_scratch`中\n",
    "我们调用了序列上的 `detach` 函数。\n",
    "为了能够快速构建模型并了解其工作原理，所有这些都没有得到充分的解释。\n",
    "在本节中，\n",
    "我们将更深入地研究一下\n",
    "详细介绍了序列模型的反向传播，以及数学原理。\n",
    "\n",
    "当我们第一次实现rnn(:numref:`sec_rnn_scratch`)时，我们遇到了梯度爆炸的一些影响。\n",
    "特别是，\n",
    "如果你解决了这些练习，\n",
    "您会看到，梯度剪裁对于确保适当的融合是至关重要的。\n",
    "为了更好地理解这一问题，本文将回顾如何计算序列模型的梯度。\n",
    "笔记\n",
    "它的工作原理在概念上没有什么新的东西。 毕竟，我们仍然只是应用链式规则来计算梯度。 尽管如此，情况确实如此\n",
    "再次查看反向传播（：numref:`sec_backprop`）时值得。\n",
    "\n",
    "\n",
    "我们已经描述了向前和向后传播\n",
    "和计算图\n",
    "在MLPs中 :numref:`sec_backprop`。\n",
    "RNN中的前向传播是相对稳定\n",
    "易懂的\n",
    "*通过时间*的反向传播实际上是一个特定的过程\n",
    "反向传播的应用\n",
    "在RNNs中 :cite:`Werbos.1990`。\n",
    "它要求我们将RNN的计算图每一步展开，以获得模型变量和参数之间的依赖关系。\n",
    "然后，\n",
    "根据链式法则，\n",
    "我们将反向传播应用于计算和分析\n",
    "存储梯度。\n",
    "由于序列可能相当长，因此依赖关系可能相当长。\n",
    "例如，对于1000个字符的序列， \n",
    "第一个标记可能会对最终位置的标记产生重大影响。\n",
    "这在计算上并不可行\n",
    "（这需要太长的时间和太多的内存）并且需要1000多个矩阵积，我们才能达到非常难以捉摸的梯度。\n",
    "这是一个充满计算和统计不确定性的过程。\n",
    "下面我们将阐明发生了什么\n",
    "以及如何在实践中解决这一问题。\n",
    "\n",
    "## RNN中的梯度分析\n",
    ":label:`subsec_bptt_analysis`\n",
    "\n",
    "我们从RNN工作原理的简化模型开始。\n",
    "此模型忽略有关隐藏状态的细节以及如何更新隐藏状态的详细信息。\n",
    "这里的数学符号\n",
    "没有明确区分\n",
    "标量， 向量， 和以前一样的矩阵。\n",
    "这些细节对分析不重要\n",
    "只会使符号变得混乱\n",
    "\n",
    "在这个简化模型中，\n",
    "我们表示 $h_t$ 作为隐藏状态，\n",
    "$x_t$ 作为输入， 和 $o_t$ 作为输出\n",
    "偶而 $t$。\n",
    "回想一下我们在\n",
    ":numref:`subsec_rnn_w_hidden_states`\n",
    "输入和隐藏状态\n",
    "可以连接到\n",
    "乘以隐藏层中的一个权重变量。\n",
    "因此，我们使用 $w_h$ 和 $w_o$ 来\n",
    "分别指示隐藏层和输出层的权重。\n",
    "因此，每一步骤的隐藏状态和输出可以解释为\n",
    "\n",
    "$$\\begin{aligned}h_t &= f(x_t, h_{t-1}, w_h),\\\\o_t &= g(h_t, w_o),\\end{aligned}$$\n",
    ":eqlabel:`eq_bptt_ht_ot`\n",
    "\n",
    "其中，$f$和$g$分别是隐藏层和输出层的转换。\n",
    "因此，我们有一个价值观 $\\{\\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \\ldots\\}$ 通过循环计算相互依赖。\n",
    "正向传播相当简单。\n",
    "我们所需要的就是循环通过三次 $（x_t，h_t，o_t）$ 每一步骤。\n",
    "输出 $o_t$ 和所需标签 $y_t$ 之间的差异由目标函数进行评估\n",
    "跨越所有的 $T$ 时间\n",
    "像\n",
    "$$L(x_1, \\ldots, x_T, y_1, \\ldots, y_T, w_h, w_o) = \\frac{1}{T}\\sum_{t=1}^T l(y_t, o_t).$$\n",
    "\n",
    "\n",
    "\n",
    "对于反向传播，问题有点棘手，特别是当我们计算关于目标函数 $L$ 的参数 $w_h$ 的梯度时。具体来说，根据链式规则，\n",
    "\n",
    "$$\\begin{aligned}\\frac{\\partial L}{\\partial w_h}  & = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_h}  \\\\& = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_h)}{\\partial h_t}  \\frac{\\partial h_t}{\\partial w_h}.\\end{aligned}$$\n",
    ":eqlabel:`eq_bptt_partial_L_wh`\n",
    "\n",
    "第一个和第二个因素\n",
    "中的乘积:eqref:`eq_bptt_partial_L_wh`\n",
    "很容易计算。\n",
    "第三个因素 $\\partial h_t/\\partial w_h$ 这就是事情变得棘手的地方，因为我们需要反复计算参数 $w_h$ 对 $h_t$ 的影响。\n",
    "根据递归计算\n",
    ":eqref:`eq_bptt_ht_ot`,\n",
    "$h_t$ 同时依赖于 $h_{t-1}$ 和 $w_h$,\n",
    "其中，$h_{t-1}$ 的计算\n",
    "还取决于 $w_h$。\n",
    "因此\n",
    "使用链式法则可以得出\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial w_h}= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}.$$\n",
    ":eqlabel:`eq_bptt_partial_ht_wh_recur`\n",
    "\n",
    "\n",
    "为了推导上面的梯度，假设我们有三个序列 $\\{a_{t}\\},\\{b_{t}\\},\\{c_{t}\\}$ 满足\n",
    "$a_{0}=0$ and $a_{t}=b_{t}+c_{t}a_{t-1}$ for $t=1, 2,\\ldots$.\n",
    "那么 $t\\geq 1$，很容易展示\n",
    "\n",
    "$$a_{t}=b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}.$$\n",
    ":eqlabel:`eq_bptt_at`\n",
    "\n",
    "将 $a_t$, $b_t$, 和 $c_t$ 替换为\n",
    "根据\n",
    "\n",
    "$$\\begin{aligned}a_t &= \\frac{\\partial h_t}{\\partial w_h},\\\\\n",
    "b_t &= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h}, \\\\\n",
    "c_t &= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}},\\end{aligned}$$\n",
    "\n",
    "网格中的梯度计算 :eqref:`eq_bptt_partial_ht_wh_recur` 满足\n",
    "$a_{t}=b_{t}+c_{t}a_{t-1}$.\n",
    "因此，\n",
    "per :eqref:`eq_bptt_at`,\n",
    "我们可以在 :eqref:`eq_bptt_partial_ht_wh_recur` 中删除递归计算\n",
    "具有\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial w_h}=\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t} \\frac{\\partial f(x_{j},h_{j-1},w_h)}{\\partial h_{j-1}} \\right) \\frac{\\partial f(x_{i},h_{i-1},w_h)}{\\partial w_h}.$$\n",
    ":eqlabel:`eq_bptt_partial_ht_wh_gen`\n",
    "\n",
    "虽然我们可以使用链规则递归地计算 $\\partial h_t/\\partial w_h$ 但只要 $t$ 很长，这个链就会变得很长。让我们讨论一些处理这个问题的策略。\n",
    "\n",
    "### 完整计算 ### \n",
    "\n",
    "明显地，\n",
    "我们只需要计算一个完整的和\n",
    ":eqref:`eq_bptt_partial_ht_wh_gen`。\n",
    "然而\n",
    "这是非常缓慢的，梯度可能会爆炸，\n",
    "因为初始条件的细微变化可能会对结果产生很大影响。\n",
    "也就是说，我们可以看到类似于蝴蝶效应的情况，即初始条件的微小变化会导致结果的不相称变化。\n",
    "就我们想要估计的模型而言，这实际上是非常不可取的。\n",
    "毕竟，我们正在寻找能够很好地推广的稳健估计。因此，这种策略几乎从未在实践中使用过。\n",
    "\n",
    "### 截断时间步骤 ###\n",
    "\n",
    "或者，\n",
    "我们可以把总和截断成整数\n",
    ":eqref:`eq_bptt_partial_ht_wh_gen`\n",
    "在 $\\tau$ 这一步之后\n",
    "这就是我们到目前为止一直在讨论的问题，\n",
    "例如，当我们在 :numref:`sec_rnn_scratch`中分离梯度时。\n",
    "这将导致真正梯度的*近似*，只需在\n",
    "$\\partial h_{t-\\tau}/\\partial w_h$. \n",
    "实际上，这很有效。 这就是通常所说的随时间缩短的后向推进 :cite:`Jaeger.2002`.\n",
    "其后果之一是，该模型主要关注短期影响，而非长期后果。 他的方法实际上是“可取的”，因为它使估计偏向于更简单、更稳定的模型。\n",
    "\n",
    "### 随机截断 ### \n",
    "\n",
    "最后，我们可以替换 $\\partial h_t/\\partial w_h$\n",
    "由一个期望值正确但截断序列的随机变量生成。\n",
    "这是通过使用 $\\xi_t$ 序列实现的\n",
    "使用预定义的 $0 \\leq \\pi_t \\leq 1$,\n",
    "其中 $P(\\xi_t = 0) = 1-\\pi_t$ 和  $P(\\xi_t = \\pi_t^{-1}) = \\pi_t$, 因此 $E[\\xi_t] = 1$.\n",
    "我们用它来代替梯度\n",
    "$\\partial h_t/\\partial w_h$\n",
    "in :eqref:`eq_bptt_partial_ht_wh_recur`\n",
    "具有\n",
    "\n",
    "$$z_t= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}.$$\n",
    "\n",
    "\n",
    "根据 $\\xi_t$ 的定义 $E[z_t] = \\partial h_t/\\partial w_h$.\n",
    "每当 $\\xi_t = 0$ 循环计算\n",
    "在当前时间步骤终止 $t$.\n",
    "这导致了不同长度序列的加权和，其中长序列很少，但适当地过重。\n",
    "这个想法是由Tallec和Ollivier提出的\n",
    ":cite:`Tallec.Ollivier.2017`.\n",
    "\n",
    "### 比较策略\n",
    "\n",
    "![比较RNN中计算梯度的策略。自上而下：随机截断、规则截断和完整计算。](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/truncated-bptt.svg)\n",
    ":label:`fig_truncated_bptt`\n",
    "\n",
    "\n",
    ":numref:`fig_truncated_bptt` 说明了使用RNN的时间反向传播分析*时间机器*书籍的前几个字符时的三种策略：\n",
    "\n",
    "* 第一行是随机截断，将文本划分为不同长度的段。\n",
    "* 第二行是规则截断，将文本拆分为相同长度的子序列。这就是我们在RNN实验中所做的。\n",
    "* 第三行是通过时间的完全反向传播，导致计算上不可行的表达式。\n",
    "\n",
    "\n",
    "不幸的是，尽管在理论上很有吸引力，但随机截断并不比常规截断更好，这很可能是由于一些因素。\n",
    "首先，在过去的许多反向传播步骤之后，观察的效果足以在实践中捕获依赖关系。\n",
    "第二，增加的方差抵消了梯度随着步骤的增加而更加精确的事实。\n",
    "第三，我们实际上想要的是互动范围很短的模型。因此，随着时间的推移，规则截断的反向传播具有轻微的正则化效果，这是可取的。\n",
    "\n",
    "## 详细介绍了时间反向传播\n",
    "\n",
    "在讨论了一般原则之后，\n",
    "让我们详细讨论时间的反向传播。\n",
    "与\n",
    ":numref:`subsec_bptt_analysis` 分析不同,\n",
    "在下面\n",
    "我们将展示\n",
    "如何计算\n",
    "目标函数的梯度\n",
    "对于所有分解的模型参数。\n",
    "为了让事情简单化，我们考虑\n",
    "无偏差参数的RNN，\n",
    "其在隐藏层中的激活功能使用标识映射 ($\\phi(x)=x$).\n",
    "对于时间步骤 $t$,\n",
    "让单个示例输入和标签\n",
    "$\\mathbf{x}_t \\in \\mathbb{R}^d$ 和 $y_t$, 分别的 \n",
    "隐藏状态 $\\mathbf{h}_t \\in \\mathbb{R}^h$ \n",
    "以及 $\\mathbf{o}_t \\in \\mathbb{R}^q$\n",
    "计算如下：\n",
    "\n",
    "$$\\begin{aligned}\\mathbf{h}_t &= \\mathbf{W}_{hx} \\mathbf{x}_t + \\mathbf{W}_{hh} \\mathbf{h}_{t-1},\\\\\n",
    "\\mathbf{o}_t &= \\mathbf{W}_{qh} \\mathbf{h}_{t},\\end{aligned}$$\n",
    "\n",
    "where $\\mathbf{W}_{hx} \\in \\mathbb{R}^{h \\times d}$, $\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$, and\n",
    "$\\mathbf{W}_{qh} \\in \\mathbb{R}^{q \\times h}$\n",
    "是权重参数。\n",
    "用 $l(\\mathbf{o}_t, y_t)$ 表示\n",
    "时间步骤的损失为 $t$. \n",
    "我们的目标函数，\n",
    "超过 $T$ 时间步骤\n",
    "从序列的开始\n",
    "因此\n",
    "\n",
    "$$L = \\frac{1}{T} \\sum_{t=1}^T l(\\mathbf{o}_t, y_t).$$\n",
    "\n",
    "\n",
    "为了可视化\n",
    "计算过程中的模型变量和参数\n",
    "在RNN中，\n",
    "我们可以画一个模型的计算图，\n",
    "如 :numref:`fig_rnn_bptt` 所示。\n",
    "例如，计算时间步骤三的隐藏状态， $\\mathbf{h}_3$, 取决于模型参数 $\\mathbf{W}_{hx}$ 和 $\\mathbf{W}_{hh}$，\n",
    "上一时间步骤的隐藏状态 $\\mathbf{h}_2$,\n",
    "以及当前时间步骤的输入 $\\mathbf{x}_3$.\n",
    "\n",
    "![显示具有三个时间步的RNN模型依赖关系的计算图。框表示变量（没有颜色色）或参数（有颜色），圆表示运算符。](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/rnn-bptt.svg)\n",
    ":label:`fig_rnn_bptt`\n",
    "\n",
    "如前所述，模型参数 :numref:`fig_rnn_bptt` $\\mathbf{W}_{hx}$, $\\mathbf{W}_{hh}$， 和 $\\mathbf{W}_{qh}$。\n",
    "通常地\n",
    "训练这个模型\n",
    "要求\n",
    "关于这些参数的梯度计算\n",
    "$\\partial L/\\partial \\mathbf{W}_{hx}$, $\\partial L/\\partial \\mathbf{W}_{hh}$，和 $\\partial L/\\partial \\mathbf{W}_{qh}$。\n",
    "根据 :numref:`fig_rnn_bptt` 的依赖关系，\n",
    "我们可以穿越\n",
    "在箭头的相反方向\n",
    "依次计算和梯度依次变化。\n",
    "灵活表达乘法\n",
    "不同形状的矩阵、向量和标量\n",
    "在链式规则中，\n",
    "我们继续使用\n",
    "这个\n",
    "$\\text{prod}$ 运算符，如下所述\n",
    ":numref:`sec_backprop`。\n",
    "\n",
    "\n",
    "首先，\n",
    "目标函数的微分\n",
    "关于模型输出\n",
    "在任何时候步骤 $t$\n",
    "这相当简单：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{o}_t} =  \\frac{\\partial l (\\mathbf{o}_t, y_t)}{T \\cdot \\partial \\mathbf{o}_t} \\in \\mathbb{R}^q.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ot`\n",
    "\n",
    "现在，我们可以计算目标函数的梯度\n",
    "关于参数 $\\mathbf{W}_{qh}$\n",
    "在输出层中：\n",
    "$\\partial L/\\partial \\mathbf{W}_{qh} \\in \\mathbb{R}^{q \\times h}$。 基于： :numref:`fig_rnn_bptt`，\n",
    "目标函数\n",
    "$L$ 通过 $\\mathbf{W}_{qh}$ 依赖于 $\\mathbf{o}_1, \\ldots, \\mathbf{o}_T$. 使用链式法则可以得出\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{qh}}\n",
    "= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{W}_{qh}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{o}_t} \\mathbf{h}_t^\\top,\n",
    "$$\n",
    "\n",
    "其中 $\\partial L/\\partial \\mathbf{o}_t$\n",
    "由以下公式得出：eqref:`eq_bptt_partial_L_ot`。\n",
    "\n",
    "接下来，如所示：numref:`fig_rnn_bptt`,\n",
    "在最后一个时间步骤 $T$\n",
    "目标函数\n",
    "$L$ 依赖于隐藏状态 $\\mathbf{h}_T$ 仅通过 $\\mathbf{o}_T$.\n",
    "因此，我们很容易找到\n",
    "梯度\n",
    "$\\partial L/\\partial \\mathbf{h}_T \\in \\mathbb{R}^h$\n",
    "使用链式规则：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_T} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_T}, \\frac{\\partial \\mathbf{o}_T}{\\partial \\mathbf{h}_T} \\right) = \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_T}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_hT_final_step`\n",
    "\n",
    "任何时间步骤都会变得更加棘手 $t < T$,\n",
    "其中目标函数 $L$ 依赖于 $\\mathbf{h}_t$ 通过 $\\mathbf{h}_{t+1}$ 和 $\\mathbf{o}_t$.\n",
    "按照链式法则，\n",
    "隐藏的梯度\n",
    "$\\partial L/\\partial \\mathbf{h}_t \\in \\mathbb{R}^h$\n",
    "在任何时候，步骤 $t < T$ 可重复计算为：\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_t} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}}, \\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t} \\right) + \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{h}_t} \\right) = \\mathbf{W}_{hh}^\\top \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}} + \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_t}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ht_recur`\n",
    "\n",
    "作为分析，\n",
    "扩展递归计算\n",
    "对于任何时间步骤 $1 \\leq t \\leq T$\n",
    "给出\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_{hh}^\\top\\right)}^{T-i} \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_{T+t-i}}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ht`\n",
    "\n",
    "我们可以从中看到：eqref:`eq_bptt_partial_L_ht` that\n",
    "这是一个简单的线性例子\n",
    "展示了长序列模型的一些关键问题：它可能涉及 $\\mathbf{W}_{hh}^\\top$的非常大的幂次。\n",
    "其中，小于1的特征值消失\n",
    "特征值大于1的偏离\n",
    "这在数值上是不稳定的，\n",
    "以消失的形式表现出来\n",
    "和爆炸梯度。\n",
    "解决这个问题的一种方法是截断时间步骤\n",
    "如：numref:`subsec_bptt_analysis`. \n",
    "实际上，这种截断是通过在给定的时间步骤数后分离梯度来实现的。\n",
    "过后\n",
    "我们将看到更复杂的序列模型，如长-短期记忆，如何进一步缓解这种情况。\n",
    "\n",
    "最后，\n",
    ":numref:`fig_rnn_bptt` 表明\n",
    "目标函数\n",
    "$L$ 取决于模型参数\n",
    "$\\mathbf{W}_{hx}$ and $\\mathbf{W}_{hh}$\n",
    "在隐藏层中\n",
    "通过隐藏状态\n",
    "$\\mathbf{h}_1, \\ldots, \\mathbf{h}_T$.\n",
    "计算梯度\n",
    "关于这些参数\n",
    "$\\partial L / \\partial \\mathbf{W}_{hx} \\in \\mathbb{R}^{h \\times d}$ 和 $\\partial L / \\partial \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$,\n",
    "我们应用链式法则，给出\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{hx}}\n",
    "&= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_{hx}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{x}_t^\\top,\\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{hh}}\n",
    "&= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_{hh}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{h}_{t-1}^\\top,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "哪里\n",
    "$\\partial L/\\partial \\mathbf{h}_t$\n",
    "由\n",
    ":eqref:`eq_bptt_partial_L_hT_final_step`\n",
    "和\n",
    ":eqref:`eq_bptt_partial_L_ht_recur`\n",
    "关键数量是多少\n",
    "这会影响数值稳定性。\n",
    "\n",
    "\n",
    "\n",
    "由于时间的反向传播\n",
    "是反向传播在RNN中的应用，\n",
    "正如我们在：numref:`sec_backprop`，\n",
    "训练RNN。\n",
    "交替向前传播\n",
    "随时间的反向传播。\n",
    "此外，\n",
    "时间反向传播\n",
    "计算并存储上述梯度\n",
    "反过来\n",
    "明确地，\n",
    "存储中间值\n",
    "重复使用\n",
    "为避免重复计算，\n",
    "比如储存\n",
    "$\\partial L/\\partial \\mathbf{h}_t$\n",
    "用于计算两者 $\\partial L / \\partial \\mathbf{W}_{hx}$ and $\\partial L / \\partial \\mathbf{W}_{hh}$。\n",
    "\n",
    "\n",
    "## 总结\n",
    "\n",
    "* 时间反向传播仅仅是反向传播对具有隐藏状态的序列模型的应用。\n",
    "* 为了计算方便和数值稳定性，需要进行截断，如正则截断和随机截断。\n",
    "* 矩阵的高次幂会导致特征值不同或突然消失。这表现为爆炸或消失的梯度形式。\n",
    "* 为了提高计算效率，在时间反向传播期间缓存中间值。\n",
    "\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 假设我们有一个对称矩阵 $\\mathbf{M} \\in \\mathbb{R}^{n \\times n}$ 特征值为 $\\lambda_i$ 其对应的特征向量为 $\\mathbf{v}_i$ ($i = 1, \\ldots, n$). 在不丢失一般性的情况下，假设它们按 $|\\lambda_i| \\geq |\\lambda_{i+1}|$ 顺序排列。\n",
    "   1. 证明 $\\mathbf{M}^k$ 具有特征值 $\\lambda_i^k$.\n",
    "   1. 证明对于一个随机向量 $\\mathbf{x} \\in \\mathbb{R}^n$, 极有可能 $\\mathbf{M}^k \\mathbf{x}$ 将与特征向量非常一致 $\\mathbf{v}_1$ 的\n",
    " $\\mathbf{M}$。把这句话正式化。\n",
    "   1. 上述结果对RNN中的梯度意味着什么？\n",
    "1. 除了梯度裁剪外，你能想出其他方法来处理递归神经网络中的梯度爆炸吗？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "14.0.2+12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
