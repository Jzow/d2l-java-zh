{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 文本预处理\n",
    ":label:`sec_text_preprocessing`\n",
    "\n",
    "我们进行了审查和评估\n",
    "统计工具\n",
    "和预测挑战\n",
    "对于序列数据。\n",
    "这些数据可以采取多种形式。\n",
    "明确地\n",
    "我们将重点关注\n",
    "在这本书的许多章节中，\n",
    "文本是最流行的序列数据示例之一。\n",
    "例如\n",
    "一篇文章可以简单地看作是一系列单词，甚至是一系列字符。\n",
    "为了方便我们将来的实验\n",
    "使用序列数据，\n",
    "我们将专门介绍这一节\n",
    "解释文本的常见预处理步骤。\n",
    "通常，这些步骤是：\n",
    "\n",
    "1. 将文本作为字符串加载到内存中。\n",
    "2. 将字符串拆分为标记（例如，单词和字符）。\n",
    "3. 建立词汇表，将分割标记映射到数字索引。\n",
    "4. 将文本转换为数字索引序列，以便模型轻松操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../utils/djl-imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 读取数据集\n",
    "\n",
    "首先，我们从H.G.Wells的[*时间机器*]加载文本(http://www.gutenberg.org/ebooks/35).\n",
    "这是一个只有30000多个单词的相当小的语料库，但是为了我们想要说明的目的，这是很好的。\n",
    "更现实的文档集合包含数十亿字。\n",
    "下面的函数将数据集读入文本行列表，其中每一行都是一个字符串。\n",
    "为了简单起见，这里我们忽略了标点符号和大写字母。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public String[] readTimeMachine() throws IOException {\n",
    "    URL url = new URL(\"http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt\");\n",
    "    String[] lines;\n",
    "    try (BufferedReader in = new BufferedReader(new InputStreamReader(url.openStream()))) {\n",
    "        lines = in.lines().toArray(String[]::new);\n",
    "    }\n",
    "\n",
    "    for (int i = 0; i < lines.length; i++) {\n",
    "        lines[i] = lines[i].replaceAll(\"[^A-Za-z]+\", \" \").strip().toLowerCase();\n",
    "    }\n",
    "    return lines;\n",
    "}\n",
    "\n",
    "String[] lines = readTimeMachine();\n",
    "System.out.println(\"# text lines: \" + lines.length);\n",
    "System.out.println(lines[0]);\n",
    "System.out.println(lines[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 标记化\n",
    "\n",
    "下面的`tokenize` 函数\n",
    "将数组（`lines`）作为输入，\n",
    "其中，每个元素都是一个文本序列（例如，文本行）。\n",
    "每个文本序列被拆分为一个标记列表。\n",
    "*标记*是文本中的基本单位。\n",
    "最后\n",
    "返回token列表的列表，\n",
    "其中每个标记都是一个字符串。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public String[][] tokenize(String[] lines, String token) throws Exception {\n",
    "    // 将文本行拆分为单词或字符标记\n",
    "    String[][] output = new String[lines.length][];\n",
    "    if (token == \"word\") {\n",
    "        for (int i = 0; i < output.length; i++) {\n",
    "            output[i] = lines[i].split(\" \");\n",
    "        }\n",
    "    } else if (token == \"char\") {\n",
    "        for (int i = 0; i < output.length; i++) {\n",
    "            output[i] = lines[i].split(\"\");\n",
    "        }\n",
    "    } else {\n",
    "        throw new Exception(\"ERROR: unknown token type: \" + token);\n",
    "    }\n",
    "    return output; \n",
    "}\n",
    "String[][] tokens = tokenize(lines, \"word\");\n",
    "for (int i = 0; i < 11; i++) {\n",
    "    System.out.println(Arrays.toString(tokens[i]));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## 词汇\n",
    "\n",
    "token的字符串类型不便于模型使用，因为模型需要数字输入。\n",
    "现在，让我们构建一个字典（HashMap），通常也被称为*词汇*，将字符串标记映射到从0开始的数字索引中。\n",
    "为此，我们首先统计训练集中所有文档中的唯一标记，\n",
    "即 *语料库*，\n",
    "然后根据每个唯一标记的频率为其分配一个数字索引。\n",
    "很少出现的标记通常会被移除以降低复杂性。\n",
    "语料库中不存在或已删除的任何标记都映射到一个特殊的未知标记“&lt;unk&gt;”。\n",
    "我们可以选择添加保留令牌的列表，例如\n",
    "“&lt;pad&gt;” 对于填充，\n",
    "“&lt;bos&gt;” 显示序列的开头，以及“&lt;eos&gt;”用于序列的结尾。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public class Vocab {\n",
    "    public int unk;\n",
    "    public List<Map.Entry<String, Integer>> tokenFreqs;\n",
    "    public List<String> idxToToken;\n",
    "    public HashMap<String, Integer> tokenToIdx;\n",
    "\n",
    "    public Vocab(String[][] tokens, int minFreq, String[] reservedTokens) {\n",
    "        // 按频率排序\n",
    "        LinkedHashMap<String, Integer> counter = countCorpus2D(tokens);\n",
    "        this.tokenFreqs = new ArrayList<Map.Entry<String, Integer>>(counter.entrySet()); \n",
    "        Collections.sort(tokenFreqs, \n",
    "            new Comparator<Map.Entry<String, Integer>>() { \n",
    "                public int compare(Map.Entry<String, Integer> o1, Map.Entry<String, Integer> o2) { \n",
    "                    return (o2.getValue()).compareTo(o1.getValue()); \n",
    "                }\n",
    "            });\n",
    "        \n",
    "        // 未知标记的索引为0\n",
    "        this.unk = 0;\n",
    "        List<String> uniqTokens = new ArrayList<>();\n",
    "        uniqTokens.add(\"<unk>\");\n",
    "        Collections.addAll(uniqTokens, reservedTokens);\n",
    "        for (Map.Entry<String, Integer> entry : tokenFreqs) {\n",
    "            if (entry.getValue() >= minFreq && !uniqTokens.contains(entry.getKey())) {\n",
    "                uniqTokens.add(entry.getKey());\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        this.idxToToken = new ArrayList<>();\n",
    "        this.tokenToIdx = new HashMap<>();\n",
    "        for (String token : uniqTokens) {\n",
    "            this.idxToToken.add(token);\n",
    "            this.tokenToIdx.put(token, this.idxToToken.size()-1);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public int length() {\n",
    "        return this.idxToToken.size();\n",
    "    }\n",
    "    \n",
    "    public Integer[] getIdxs(String[] tokens) {\n",
    "        List<Integer> idxs = new ArrayList<>();\n",
    "        for (String token : tokens) {\n",
    "            idxs.add(getIdx(token));\n",
    "        }\n",
    "        return idxs.toArray(new Integer[0]);\n",
    "        \n",
    "    }\n",
    "    \n",
    "    public Integer getIdx(String token) {\n",
    "        return this.tokenToIdx.getOrDefault(token, this.unk);\n",
    "    }\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "public LinkedHashMap<String, Integer> countCorpus(String[] tokens) {\n",
    "    /* 计算token频率 */\n",
    "    LinkedHashMap<String, Integer> counter = new LinkedHashMap<>();\n",
    "    if (tokens.length != 0) {\n",
    "        for (String token : tokens) {\n",
    "            counter.put(token, counter.getOrDefault(token, 0)+1);\n",
    "        }\n",
    "    }\n",
    "    return counter;\n",
    "}\n",
    "\n",
    "public LinkedHashMap<String, Integer> countCorpus2D(String[][] tokens) {\n",
    "    /* 将token列表展平为token列表*/\n",
    "    List<String> allTokens = new ArrayList<String>();\n",
    "    for (int i = 0; i < tokens.length; i++) {\n",
    "        for (int j = 0; j < tokens[i].length; j++) {\n",
    "             if (tokens[i][j] != \"\") {\n",
    "                allTokens.add(tokens[i][j]);\n",
    "             }\n",
    "        }\n",
    "    }\n",
    "    return countCorpus(allTokens.toArray(new String[0]));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "我们使用时间机器数据集作为语料库构建了一个词汇表。\n",
    "然后，我们打印前几个频繁标记及其索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab vocab = new Vocab(tokens, 0, new String[0]);\n",
    "for (int i = 0; i < 10; i++) {\n",
    "    String token = vocab.idxToToken.get(i);\n",
    "    System.out.print(\"(\" + token + \", \" + vocab.tokenToIdx.get(token) + \") \");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "现在我们可以将每一行文本转换为一个数字索引列表。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (int i : new int[] {0,10}) {\n",
    "    System.out.println(\"Words:\" + Arrays.toString(tokens[i]));\n",
    "    System.out.println(\"Indices:\" + Arrays.toString(vocab.getIdxs(tokens[i])));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## 把所有的东西放在一起\n",
    "\n",
    "使用上述函数，我们将所有内容打包到`loadCorpusTimeMachine`函数中，\n",
    "该函数返回`corpus`，一个标记索引列表，以及`vocab`，时间机器语料库的词汇表。\n",
    "我们在这里做的修改是：\n",
    "一） 我们将文本标记为字符，而不是单词，以简化后面章节中的训练；\n",
    "二）`corpus` 是一个单一的列表，而不是标记列表列表，因为时间机器数据集中的每一行文本不一定是一个句子或段落。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "public Pair<List<Integer>, Vocab> loadCorpusTimeMachine(int maxTokens) throws IOException, Exception {\n",
    "    /* 返回时间机器数据集的令牌索引和词汇表。 */\n",
    "    String[] lines = readTimeMachine();\n",
    "    String[][] tokens = tokenize(lines, \"char\");\n",
    "    Vocab vocab = new Vocab(tokens, 0, new String[0]);\n",
    "    // 因为时间机器数据集中的每个文本行不一定是\n",
    "    // 句子或段落，将所有文本行展平为一个列表\n",
    "    List<Integer> corpus = new ArrayList<>();\n",
    "    for (int i = 0; i < tokens.length; i++) {\n",
    "        for (int j = 0; j < tokens[i].length; j++) {\n",
    "            if (tokens[i][j] != \"\") {\n",
    "                corpus.add(vocab.getIdx(tokens[i][j]));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if (maxTokens > 0) {\n",
    "        corpus = corpus.subList(0, maxTokens);\n",
    "    }\n",
    "    return new Pair(corpus, vocab);\n",
    "}\n",
    "\n",
    "Pair<List<Integer>, Vocab> corpusVocabPair = loadCorpusTimeMachine(-1);\n",
    "List<Integer> corpus = corpusVocabPair.getKey();\n",
    "Vocab vocab = corpusVocabPair.getValue();\n",
    "\n",
    "System.out.println(corpus.size());\n",
    "System.out.println(vocab.length());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## 总结\n",
    "\n",
    "* 文本是序列数据的一种重要形式。\n",
    "* 为了预处理文本，我们通常将文本拆分为标记，构建词汇表将标记字符串映射为数字索引，并将文本数据转换为标记索引，以便模型进行操作。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 标记化是一个关键的预处理步骤。它因语言而异。尝试找到另外三种常用的文本标记方法。\n",
    "2. 在本节的实验中，将文本标记为单词，并改变`Vocab`实例的 `minFreq` 参数。这是如何影响词汇量的？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "14.0.2+12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
